import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


np.random.seed(0)
n = 15
x = np.linspace(0,10,n) + np.random.randn(n)/5
y = np.sin(x)+x/6 + np.random.randn(n)/10


X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)

#Question 1
#Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 1, 3, 6, and 9. (Use PolynomialFeatures in sklearn.preprocessing to create the polynomial features and then fit a linear regression model) For each model, find 100 predicted values over the interval x = 0 to 10 (e.g. np.linspace(0,10,100)) and store this in a numpy array. The first row of this array should correspond to the output from the model trained on degree 1, the second row degree 3, the third row degree 6, and the fourth row degree 9.

from sklearn.linear_model import LogisticRegression
dataset = np.linspace(0,10,100)
data_shpd = dataset.reshape(100, 1)
func_res = np.zeros((4,100))
def answer_one():
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    list_degrees = [1,3,6,9]
    for counter, val in enumerate(list_degrees):
        model = PolynomialFeatures(degree=val)
        X_poly = model.fit_transform(X_train.reshape(11,1))
        model2 = LinearRegression().fit(X_poly, y_train)
        prediction = model2.predict(model.fit_transform(data_shpd))
        func_res[counter] = prediction
        
    return func_res
answer_one()
    
#Question 2
#Write a function that fits a polynomial LinearRegression model on the training data X_train for degrees 0 through 9. For each model compute the  R2
 #(coefficient of determination) regression score on the training data as well as the the test data, and return both of these arrays in a tuple.

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics.regression import r2_score


def answer_two():
    r2_train = np.zeros(10,)
    r2_test = np.zeros(10,)
    
    for i in range(10):

        model = PolynomialFeatures(degree=i)
        
        
        X_poly_train = model.fit_transform(X_train.reshape(11,1))
        linreg = LinearRegression().fit(X_poly_train, y_train)
        
        r2_train_degree = r2_score(X_train, linreg.predict(X_poly_train))
        
        X_poly_test = model.fit_transform(X_test.reshape(4,1))
      
        r2_test_degree = r2_score(X_test, linreg.predict(X_poly_test))
        
        r2_train[i] = r2_train_degree
        r2_test[i] = r2_test_degree
        scores = (r2_train, r2_test)
  
    return scores
answer_two()

##Question 3
#Based on the  R2
 # scores from question 2 (degree levels 0 through 9), what degree level corresponds to a model that is underfitting? What degree level corresponds to a model that is overfitting? What choice of degree level would provide a model with good generalization performance on this dataset?
 
 def answer_three():
    
    
    return (1,9, 6)
    
answer_three()


##Question 4
#Training models on high degree polynomial features can result in overly complex models that overfit, so we often use regularized versions of the model to constrain model complexity, as we saw with Ridge and Lasso linear regression.

def answer_four():
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import Lasso, LinearRegression
    from sklearn.metrics.regression import r2_score
    

    model = PolynomialFeatures(degree=12) 
    X_poly_train = model.fit_transform(X_train.reshape(11,1))
    X_poly_test = model.fit_transform(X_test.reshape(4,1))
    
    linreg = LinearRegression().fit(X_poly_train, y_train)
    LinearRegression_R2_test_score = linreg.score(X_poly_test, y_test)
    
    lasso_reg = Lasso(alpha=0.01, max_iter = 10000).fit(X_poly_train, y_train)
    Lasso_R2_test_score = lasso_reg.score(X_poly_test, y_test)

    return (LinearRegression_R2_test_score, Lasso_R2_test_score)
answer_four()

##Part 2 - Classification
#Here's an application of machine learning that could save your life! For this section of the assignment we will be working with the UCI Mushroom Data Set stored in readonly/mushrooms.csv. The data will be used to train a model to predict whether or not a mushroom is poisonous. The following attributes are provided:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split



mush_df = pd.read_csv('mushrooms.csv')

mush_df2 = pd.get_dummies(mush_df)



X_mush = mush_df2.iloc[:,2:]
y_mush = mush_df2.iloc[:,1]

# use the variables X_train2, y_train2 for Question 5
X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)

# For performance reasons in Questions 6 and 7, we will create a smaller version of the
# entire mushroom dataset for use in those questions.  For simplicity we'll just re-use
# the 25% test split created above as the representative subset.
#
# Use the variables X_subset, y_subset for Questions 6 and 7.
X_subset = X_test2
y_subset = y_test2


##Question 5
#Using X_train2 and y_train2 from the preceeding cell, train a DecisionTreeClassifier with default parameters and random_state=0. What are the 5 most important features found by the decision tree?

def answer_five():
    #import matplotlib.pyplot as plt
    from sklearn.tree import DecisionTreeClassifier
    clf = DecisionTreeClassifier(random_state= 0).fit(X_train2, y_train2)
    from adspy_shared_utilities import plot_feature_importances
    #print(X_train2.columns)
    mushroom_feature_names = X_train2.columns
    #plt.figure(figsize=(10,4), dpi=80)
    #plot_feature_importances(clf, mushroom_feature_names)
    #plt.show()
    #get importance
    importance = clf.feature_importances_
    feature_names = X_train2.columns
    feature_names_importance = dict()
    for i,v in enumerate(importance):
        feature_names_importance.update({feature_names[i]: v})
    sorted_importance = sorted(feature_names_importance.items(), key=lambda item: item[1], reverse=True)
    five_important_values = list()
    for i in range(5):
        name, val = sorted_importance[i]
        five_important_values.append(name)
    return five_important_values # Your answer here
answer_five()
        
##Question 6
#For this question, we're going to use the validation_curve function in sklearn.model_selection to determine training and test scores for a Support Vector Classifier (SVC) with varying parameter values. Recall that the validation_curve function, in addition to taking an initialized unfitted classifier object, takes a dataset as input and does its own internal train-test splits to compute results.

def answer_six():
    from sklearn.svm import SVC
    from sklearn.model_selection import validation_curve
    model = SVC(kernel='rbf', C=1,random_state = 0)
    training_scores, test_scores = validation_curve(model, X_subset, y_subset, param_name = "gamma", param_range= np.logspace(-4,1,6), scoring='accuracy')
    
    # Your code here

    return training_scores.mean(axis=1), test_scores.mean(axis=1) # Your answer here
answer_six()

##Question 7
#Based on the scores from question 6, what gamma value corresponds to a model that is underfitting (and has the worst test set accuracy)? What gamma value corresponds to a model that is overfitting (and has the worst test set accuracy)? What choice of gamma would be the best choice for a model with good generalization performance on this dataset (high accuracy on both training and test set)?

def answer_seven():
    #print(answer_six())
    # Your code here
    return (0.001, 10, 0.1)# Return your answer
answer_seven()
