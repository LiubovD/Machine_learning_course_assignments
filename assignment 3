##Assignment 3 - Evaluation
#In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on dataset from Kaggle.
#Each row in fraud_data.csv corresponds to a credit card transaction. Features include confidential variables V1 through V28 as well as Amount which is the amount of the transaction.

##Question 1
#Import the data from fraud_data.csv. What percentage of the observations in the dataset are instances of fraud?

import numpy as np
import pandas as pd

frd = pd.read_csv('fraud_data.csv') 

def answer_one():
    val_cnt = frd["Class"].value_counts()
    sumfrd = 0
    nbr_frd = 0
    for count, value in enumerate(val_cnt):
        sumfrd += value
        if count == 1:
            nbr_frd = value
    prcnt_frd = nbr_frd/sumfrd

    return prcnt_frd

answer_one()

# Use X_train, X_test, y_train, y_test for all of the following questions
from sklearn.model_selection import train_test_split

df = pd.read_csv('fraud_data.csv')

X = df.iloc[:,:-1]
y = df.iloc[:,-1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

##Question 2
#Using X_train, X_test, y_train, and y_test (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?

from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score

def answer_two():
    from sklearn.dummy import DummyClassifier
    from sklearn.metrics import recall_score
    dummy_majority = DummyClassifier().fit(X_train, y_train)
    y_pred = dummy_majority.predict(X_test)
    acc_sc = accuracy_score(y_test, y_pred)
    re_sc = recall_score(y_test, y_pred)
    return acc_sc, re_sc
answer_two()

##Question 3
#Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?
#This function should a return a tuple with three floats, i.e. (accuracy score, recall score, precision score).

from sklearn.metrics import precision_score

def answer_three():
    from sklearn.metrics import recall_score, precision_score
    from sklearn.svm import SVC

    svm = SVC().fit(X_train, y_train)
    svm_predicted = svm.predict(X_test)
    acc_sc = accuracy_score(y_test, svm_predicted)
    re_sc = recall_score(y_test, svm_predicted)
    prc_sc = precision_score(y_test, svm_predicted)
    
    return acc_sc, re_sc, prc_sc
answer_three()

##Question 4
#Using the SVC classifier with parameters {'C': 1e9, 'gamma': 1e-07}, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.

def answer_four():
    from sklearn.metrics import confusion_matrix
    from sklearn.svm import SVC

    svm = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)
    y_score = svm.decision_function(X_test)
    y_score[y_score > -220] = 1
    y_score[y_score < -220] = 0
    y_score[y_score == -220] = 0
    conf_matrix = confusion_matrix(y_test, y_score)
    
    
    return conf_matrix
answer_four()


##Question 5
#Train a logisitic regression classifier with default parameters using X_train and y_train.
#For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).
#Looking at the precision recall curve, what is the recall when the precision is 0.75?
#Looking at the roc curve, what is the true positive rate when the false positive rate is 0.16?

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.dummy import DummyRegressor
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc


def answer_five():
    lm = LinearRegression().fit(X_train, y_train)
    y_predict = lm.predict(X_test)
    y_score = lm.decision_function(X_test)
    precision, recall, thresholds = precision_recall_curve(y_test, y_score)
    count_prec_def = 0
    for count, value in enumerate(precision):
        if value == 0.75:
            count_prec_def = count
    value_recall = 0
    for count, value in enumerate(recall):
        if count == count_prec_def:
            value_recall = value
        
    true_pos = 0.9
    return value_recall, true_pos
answer_five()

## Question 6
#Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.
#'penalty': ['l1', 'l2']
#'C':[0.01, 0.1, 1, 10, 100]

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

def answer_six():    
    lm = LogisticRegression()
    grid_values = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}

    grid_search = GridSearchCV(lm, param_grid=grid_values)
    grid_search.fit(X_train, y_train)
    result = grid_search.cv_results_
    mean_test_score = result['mean_test_score']

    result_corr = np.array(mean_test_score).reshape(5,2)
    return result_corr

answer_six()

